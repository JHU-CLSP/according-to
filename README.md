# Prompting LLMs to Quote from Pre-Training Data
**TLDR:** Can one prompt LLMs to ground responses against previously observed text in their pre-training? We find evidence in support of this! 

# Code to reproduce
Coming soon! 

# Further reading: 
Check the following paper for further details: 

```bibtex 
@article{weller2023accordingto,
  title={{“According to ... ”: Prompting Language Models Improves Quoting from Pre-Training Data}},
  author={Weller, Orion and Marone, Marc and Weir, Nathaniel and Lawrie, Dawn and Khashabi, Daniel and Van Durme, Benjamin },
  journal={arXiv preprint},
  year={2023}
}
```

